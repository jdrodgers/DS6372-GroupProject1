---
title: "Project_1"
author: "Charles, Lei, John"
date: "2/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(leaps)
library(caret)
library(car)


#Set path to housing data.
# JR - Changed path to be relative path assuming rproj file in same directory as rmd files. 
#       Should help with running it on separate workstations
Project <- "C:/Users/N1110/Desktop/train_clean_final.csv"

HousingData <- read.csv(Project)
dim(HousingData)

```
```{r}
####UPDATE 1 EDA ####
#Too many missing values, remove: in LotFrontage 259, FirePlaceQu 690, PoolQC 1453,MiscFeature 1406
# Fence 1179, actually not missing value but one level of factor, need transform. or how to deal with it??suggestion? remove for now for further analysis.
#These have less than  100 missing values, keep in dataset: Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, MasVnrArea 8


#Report NA count, put in data frame for easier display options using kable
naCountDF <-as.data.frame(sapply(HousingData, function(x) sum(length(which(is.na(x))))))
names(naCountDF) <- ("NA Count") # rename columns

# Print the NA Counts by State

library(kableExtra)
kable(naCountDF,format="html", caption="Table -NA Count (NA=blanks)",align='c') %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

```

####EDA, find out the column index for "FireplaceQu"
```{r}
#str(HousingData)

which( colnames(HousingData)=="FireplaceQu" )

```
####Removes unnessary predictor columns from dataset
```{r}

Housing <- HousingData[, -c(1,4,7,15,58,73:76,79)] #Removes unnessary predictor columns from dataset
#Features removed: Id, Alley, Condition2, PoolQC, Fense, MiscFeature, MiscVal, SaleType

#4 LotFrontage too mnay missing data  259
#FireplaceQu has need to be removed

#dim(Housing)
#(1460   71)

head(Housing)
```

####EDA
####UPDATE 2.11 LEI. 
####correlation matrix for continous varaibles, remove obvious correlated predictors. 
```{r}
#str(Housing)

#choose only numeric columns
nums <- unlist(lapply(Housing, is.numeric)) 
DFnum<- Housing[ , nums]

#head(DFnum)
#str(DFnum)
#36 predictors num

# create correlation matrix, heatmap of correlation matrix is for visual reference when selecting the important features; we can not only rely on heatmap.
cormat <- round(cor(DFnum),2)

#head(cormat)

library(reshape2)

melted_cormat <- melt(cormat)

#head(melted_cormat)


library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 

  geom_tile()+ggtitle("Pearson correlation coefficient matrix for all numeric variables")


# Get upper triangle of the correlation matrix

  get_upper_tri <- function(cormat){

    cormat[lower.tri(cormat)]<- NA

    return(cormat)

  }
upper_tri <- get_upper_tri(cormat)
upper_tri 
```

#GarageArea,GarageCars are highly correlated, remove one of them
```{r}
attach(DFnum)
cor(GarageArea,GarageCars)
#[1] 0.8824754


```
#
```{r}

#str(Housing)

#which( colnames(Housing)=="GarageArea" )
#[1] 58

#pairs(DFnum)

Housing<-Housing[,-58]
str(Housing)

```

##linearity of predictors with LogSalePrice
```{r}
###pick 15 predictors by visually have linearity with LogSalePrice,

#OverallQual, OverallCond, YearBuilt, YearRemodAdd,MasVnrArea, TotRmsAbvGrd, GarageYrBlt, Neighborhood, TotalBsmtSF, X1stFlrSF,GrLivArea, GarageCars, FullBath, RoofStyle, ScreenPorch

pairs(Housing[,c(1:5,70)])
```

#
```{r}
pairs(Housing[,c(6:10,70)])
```
#
```{r}
pairs(Housing[,c(11:15,70)])
```
#
```{r}
pairs(Housing[,c(16:21,70)])
```
```{r}

```

#
```{r}
pairs(Housing[,c(22:27,70)])
```

#
```{r}
pairs(Housing[,c(28:33,70)])
```
#
```{r}
pairs(Housing[,c(34:40,70)])
```

#
```{r}
pairs(Housing[,c(41:48,70)])
```
#
```{r}
pairs(Housing[,c(49:55,70)])
```

#
```{r}
pairs(Housing[,c(56:61,70)])
```
#
```{r}
pairs(Housing[,c(62:68,70)])
```
#subset
```{r}
 
#which( colnames(Housing) %in% c("OverallQual"," OverallCond","YearBuilt","YearRemodAdd","MasVnrArea","TotRmsAbvGrd","GarageYrBlt","Neighborhood","TotalBsmtSF","X1stFlrSF", "GrLivArea","GarageCars","FullBath","RoofStyle","ScreenPorch","LogSalePrice"))

#which( colnames(Housing)==c("OverallQual"," OverallCond"))
#which( colnames(Housing)==c("YearRemodAdd","MasVnrArea","TotRmsAbvGrd","GarageYrBlt","Neighborhood","TotalBsmtSF","X1stFlrSF", "GrLivArea","GarageCars","FullBath","RoofStyle","ScreenPorch"))

Housing2= Housing[,which( colnames(Housing) %in% c("OverallQual","OverallCond","YearBuilt","YearRemodAdd","MasVnrArea","TotRmsAbvGrd","GarageYrBlt","Neighborhood","TotalBsmtSF","X1stFlrSF", "GrLivArea","GarageCars","FullBath","RoofStyle","ScreenPorch","LogSalePrice"))]
str(Housing2)
```
#
```{r}
pairs(Housing2[,1:5])
```

#
```{r}
#OverallQual colinearity with YearRemodAdd, remove OverallQual
pairs(Housing2[,1:5])

```

#
```{r}
 #TotalBsmtSF, X1stFlrSF,GrLivArea, are highly correlated, remove TotalBsmtSF, X1stFlrSF
pairs(Housing2[,6:11])
```

#
```{r}
pairs(Housing2[,12:15])
```


#
```{r}
###pick 15 predictors by visually have linearity with LogSalePrice,

#OverallQual, OverallCond, YearBuilt, YearRemodAdd,MasVnrArea, TotRmsAbvGrd, GarageYrBlt, Neighborhood, TotalBsmtSF, X1stFlrSF,GrLivArea, GarageCars, FullBath, RoofStyle, ScreenPorch

model.custom=lm(LogSalePrice~OverallCond+ YearBuilt+YearRemodAdd+MasVnrArea+TotRmsAbvGrd+GarageYrBlt+Neighborhood+GrLivArea+GarageCars+ FullBath+RoofStyle+ScreenPorch, data=Housing2)

plot(model.custom)
model.custom
  
```

#remove outliers 524,1299
```{r}

Housing3=Housing2[-c(524,1299),]

#str(Housing3)

model.custom2=lm(LogSalePrice~OverallCond+ YearBuilt+YearRemodAdd+MasVnrArea+TotRmsAbvGrd+GarageYrBlt+Neighborhood+GrLivArea+GarageCars+ FullBath+RoofStyle+ScreenPorch, data=Housing3)

plot(model.custom2)
model.custom2 # 12 predictors

```

#accuracy of model.cutom2
```{r}
library(forecast)
Custom.pred=predict(model.custom2 ,Housing3)
accuracy(Custom.pred,Housing3$LogSalePrice)

```

```{r Train and Test Samples}
#Following code creates training and test samples. 
set.seed(1234)
index<-sample(1:dim(Housing)[1],1460/2,replace=F)
train<-Housing[index,]
test<-Housing[-index,]

#Forward
reg.fwd<-regsubsets(SalePrice~.,data=train, method = "forward", really.big = T, nvmax = 60)

coef(reg.fwd,3)

summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic


par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:61,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:61,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:61,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)


car::vif(lm(SalePrice~HouseStyle+GrLivArea+SaleCondition,data=Housing))

housing.fwdfit<-lm(SalePrice~HouseStyle+GrLivArea+SaleCondition, data = train)

par(mfrow=c(2,2))
plot(housing.fwdfit)
```

```{r}
summary(housing.fwdfit)
```

####UPDATE 2.11 LEI. 
####log transform SalePrice, based on diagnotics (looks bad when fit the orginial data: fan out residuals, QQplot does not look normal)

```{r}
Housing$LogSalePrice=log(Housing$SalePrice)
str(Housing )
head(Housing)

```
#remove Housing$SalePrice column for further analysis
```{r}

Housing<- Housing[,-69]

head(Housing)
str(Housing)
```

#### forward model on log transformed data

```{r}
set.seed(1234)
index<-sample(1:dim(Housing)[1],1460/2,replace=F)
train<-Housing[index,]
test<-Housing[-index,]

#train <- na.omit(train)
#test <- na.omit(test)

#Forward
reg.fwd<-regsubsets(LogSalePrice~.,data=train, method = "forward", really.big = T, nvmax = 60)

coef(reg.fwd,3)

summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic


par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:61,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:61,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:61,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)


car::vif(lm(LogSalePrice~HouseStyle+GrLivArea+SaleCondition,data=Housing))

housing.fwdfit<-lm(LogSalePrice~HouseStyle+GrLivArea+SaleCondition, data = train)

par(mfrow=c(2,2))
plot(housing.fwdfit)
```

## log transformed forward
```{r}
summary(housing.fwdfit)
```

####UPDATED 2.11 LEI Number of predictors vs. ASE
#choose 30 predictors
```{r}

set.seed(1234)
index<-sample(1:dim(Housing)[1],1460/2,replace=F)
train<-Housing[index,]
test<-Housing[-index,]

train <- na.omit(train)
test <- na.omit(test)

#dim(train)
#dim(test)

#[1] 674  71
#[1] 665  71

#dim(Housing)
#[1] 1460   71

library(leaps)
reg.fwd=regsubsets(LogSalePrice~.,data=train,method="forward",nvmax=70)

#Really handy predict function
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}


testASE<-c()
#note my index is to 70 since that what I set it in regsubsets
for (i in 1:70){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=test,id=i) 
  testASE[i]<-mean((test$LogSalePrice-predictions)^2)
}

par(mfrow=c(1,1))
plot(1:70,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,0.06))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
#length(rss)
lines(1:70,rss[-1]/665,lty=3,col="blue")  #Dividing by  since ASE=RSS/sample size

```

#
```{r}
#library(forecast)
#Custom.pred=predict(reg.fwd ,Housing)
#accuracy(Custom.pred,Housing$LogSalePrice)
```

###Lasso
####UPDATE 2.11 LEI. 
####correlation matrix for continous varaibles, remove obvious one. lasso takes care of slight multicolinearity.
```{r}

set.seed(1234)
index<-sample(1:dim(Housing2)[1],1460/2,replace=F)
train<-Housing2[index,]
test<-Housing2[-index,]


train <- na.omit(train)

test <- na.omit(test)


library(glmnet)

# JR - Added line to eliminate NAs.  Not sure if correct solution, but allowed lasso to run.
# https://stackoverflow.com/questions/6447708/model-matrix-generates-fewer-rows-than-original-data-frame


#Formatting data for GLM net
x=model.matrix(LogSalePrice~.,train)[,-1]
y=log(train$LogSalePrice)


xtest<-model.matrix(LogSalePrice~.,test)[,-1]
ytest<-log(test$LogSalePrice)

#dim(train)
#str(train)
#str(x)
#str(y)

grid=10^seq(10,-2, length =730)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)


bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)
```

#
```{r}
#library(forecast)
#Custom.pred=predict(lasso.mod,Housing2)
#accuracy(Custom.pred,Housing2$LogSalePrice)
```


#str(Housing)
```{r}
#str(Housing)
```


####Update 2 two way anova####
```{r}
#create a data frame with OUTPUT SalePrice AND two catagorical predictors Neighborhood 25 levels, HouseStyle 8 levels for two way anova

#dim(Housing)
#str(Housing)
#which( colnames(HousingData)=="Neighborhood" )

#which( colnames(HousingData)=="HouseStyle" )
#which( colnames(HousingData)=="SalePrice" )

HousingANOVA <- Housing[, c(13,18,69)]  #use RoofStyle which has 6 levels to replace neighborhood with 25 levels

str(HousingANOVA )
head(HousingANOVA)

```

####UPDATE 2.11 LEI.  
#### Prepare for final HousingANOVA dataset--subset data RoofStyle with some HouseStyles
```{r}
#attach(HousingANOVAPre )
#plot(Neighborhood, HouseStyle,col=c(1:8))


attach(HousingANOVA )
plot(RoofStyle,HouseStyle,col=c(1:8))

```

####based on the graph above, subset data Neighborhoods with some HouseStyles. FINAL dataset HousingANOVA.Reduced
choose subset for two way anova

RoofStyle,
Flat, Gable, Hip

HouseStyle
2Story, 1Story, SLvl,SFoyer


delete

1.5Fin, 1.5Unf, 2.5Fin, 2.5Unf

Gambrel, Mansard, Shed


```{r}
#delete code
attach(HousingANOVA)
delete.index<- which(RoofStyle %in% c("Gambrel","Mansard","Shed"))
HousingANOVA.Redued1 <-HousingANOVA[-delete.index,]

delete.index2<- which(HousingANOVA.Redued1$HouseStyle %in% c("1.5Fin","1.5Unf","2.5Fin","2.5Unf"))
HousingANOVA.Reduced <-HousingANOVA.Redued1[-delete.index2,]

str(HousingANOVA.Redued)
```


###mean plot of transformed data
```{r}
#Provide a means plot of the data.
attach(HousingANOVA.Redued)
mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/sqrt(length(x)),min(x), max(x), IQR(x))
  names(result)<-c("N","Mean","SD","SE","MIN","MAX", "IQR")
  return(result)
}
sumstats<-aggregate(LogSalePrice~RoofStyle*HouseStyle,data=HousingANOVA.Reduced,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

ggplot(sumstats,aes(x=RoofStyle,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1)
  
#SD as error bar
  ggplot(sumstats,aes(x=RoofStyle,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("LogSalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1)
```
#fit
#Q: how to check for outlier?
```{r}
 #Fit a nonadditive 2 way anova model to transformed data and provide the residual diagnostics.
  model.fit<-aov(LogSalePrice~RoofStyle+HouseStyle+RoofStyle*HouseStyle,data=HousingANOVA.Reduced)
par(mfrow=c(1,2))
plot(model.fit$fitted.values,model.fit$residuals,ylab="Resdiduals",xlab="Fitted")
qqnorm(model.fit$residuals)

# the normality assumption met after log transformation of y

```

#pretty plot
```{r}
library(gridExtra)
myfits<-data.frame(fitted.values=model.fit$fitted.values,residuals=model.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=1,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)
```

#
```{r}
plot(model.fit)
```

####First layer of test. High level test. Provide the type 3 ANOVA F-tests. Type III SS tested on interactions (the presence of a main effect after the other main effect and interaction)
```{r}
#provide the type 3 ANOVA F-tests
library(car)
Anova(model.fit,type=3)

#looks like only RoofStyle significant
```
####Mutiple testing corrections

```{r}
TukeyHSD(model.fit,"RoofStyle",conf.level=.95)
```
##
```{r}
plot(TukeyHSD(model.fit,"RoofStyle",conf.level=.95))
```


```{r}

```

