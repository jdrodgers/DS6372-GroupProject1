---
title: "Project_1"
author: "Charles, Lei, John"
date: "2/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(leaps)
library(caret)
library(car)
library(MASS)
library(ggvis)


#Set path to housing data.
# JR - Changed path to be relative path assuming rproj file in same directory as rmd files. 
#       Should help with running it on separate workstations
Project <- "/Users/Chase/Desktop/MSDS/MSDS6306/Homework/SMU_MSDS_Homework/Case_Study_1/DS6372-GroupProject1/ImportedFiles/train_clean_final.csv"

HousingData <- read.csv(Project)
dim(HousingData)

```
```{r}
####UPDATE 1 EDA ####
#Too many missing values, remove: in LotFrontage 259, FirePlaceQu 690, PoolQC 1453,MiscFeature 1406
# Fence 1179, actually not missing value but one level of factor, need transform. or how to deal with it??suggestion? remove for now for further analysis.
#These have less than  100 missing values, keep in dataset: Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, MasVnrArea 8


#Report NA count, put in data frame for easier display options using kable
naCountDF <-as.data.frame(sapply(HousingData, function(x) sum(length(which(is.na(x))))))
names(naCountDF) <- ("NA Count") # rename columns

# Print the NA Counts by State

library(kableExtra)
kable(naCountDF,format="html", caption="Table -NA Count (NA=blanks)",align='c') %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

```

####EDA, find out the column index for "FireplaceQu"
```{r}
#str(HousingData)

which( colnames(HousingData)=="FireplaceQu" )

```
####Removes unnessary predictor columns from dataset
ID removed due to it providing no relavent descriptive feature
Alley not a feature present in enough of the samples, determined not to be a significant enough feature.
Condition2 was found to be too correlated with condition1.
PoolQC too multicollineated with Pool feature.
MiscFeature/ MiscVal presented no relavent descriptive features.
SaleType considered not relavent to SalePrice.

```{r}

Housing <- HousingData[, -c(1,4,7,15,58,73:76,79)] #Removes unnessary predictor columns from dataset
#Features removed: Id, Alley, Condition2, PoolQC, Fense, MiscFeature, MiscVal, SaleType

#4 LotFrontage too mnay missing data  259
#FireplaceQu has need to be removed

#dim(Housing)
#(1460   71)

head(Housing)
tail(Housing)
```
####EDA
```{r}
#str(Housing)
```

```{r Train and Test Samples}
#Following code creates training and test samples. 
set.seed(1234)
index<-sample(1:dim(Housing)[1],nrow(Housing)/2,replace=F)
train<-Housing[index,]
test<-Housing[-index,]


#Forward Model
reg.fwd<-regsubsets(SalePrice~.,data=train, method = "forward", really.big = T, nvmax = 20)
coef(reg.fwd, 19)


```

```{r}
#Alternate Forward Model with better results accessibility
train.control <- trainControl(method = "cv", number = 5)

forward.model <- train(SalePrice ~., data = train, method = "leapForward", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

names(results)

#The following are graphs for Feature impact on MSE, RMSE, and Rsquared based on Training Data.

#Forward Model MSE
results <- forward.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

#Forward Model RMSE
results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

#Forward Model RSquared
results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
forward.model$bestTune
##summary(forward.model$finalModel)
coef(forward.model$finalModel, 16)

##Uses other reg.fwd model to sanity check the models, same features recommended in either case. 
coef(reg.fwd, 16)

summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic

#BIC against # of Predictors/Features
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:21,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

#Adjusted RSquare against # of Predictors/Features
adjr2<-summary(reg.fwd)$adjr2
plot(1:21,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

#RSS against # of Predictors/Features
rss<-summary(reg.fwd)$rss
plot(1:21,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

```



```{r Backward}

#Backward
reg.back<-regsubsets(SalePrice~.,data=train, method = "backward", really.big = T, nvmax = 20)

coef(reg.back,3)

summary(reg.back)$adjr2
summary(reg.back)$rss
summary(reg.back)$bic

#BIC against # of Predictors/Features
par(mfrow=c(1,3))
bics<-summary(reg.back)$bic
plot(1:21,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

#Adjusted RSquare against # of Predictors/Features
adjr2<-summary(reg.back)$adjr2
plot(1:21,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

#RSS against # of Predictors/Features
rss<-summary(reg.back)$rss
plot(1:21,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)


backward.model <- train(SalePrice ~., data = train, method = "leapBackward", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

#Graphs to Feature impact on MSE, RMSE, and Rsquared based on Trainining Data.
#Backward Model MSE
back.results <- backward.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

#Backward Model RMSE
back.results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

#Backward Model RSquared
back.results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
backward.model$bestTune
##summary(forward.model$finalModel)
coef(backward.model$finalModel, 18)

##Uses other reg.fwd model to sanity check the models, same features recommended in either case. 
coef(reg.back, 18)



```


Backward regression model with 18 predictors had greatest Rsquared return in comparison with stepwise and forward. The following features were the recommended backward output.  

LotArea 
NeighborhoodNoRidge 
NeighborhoodNridgHt 
NeighborhoodStoneBr 
BldgTypeTwnhsE 
OverallQual 
OverallCond 
YearBuilt 
RoofMatlCompShg 
RoofMatlTar&Grv 
RoofMatlWdShngl 
Exterior1stBrkFace 
Exterior1stCBlock 
BsmtFinType1Unf 
BsmtFinSF1 
HeatingQCPo 
FunctionalTyp 
Fireplaces

Original output contains too many features. Fear of overtraining the model. First step was to consolidate the recommended features that were either single variables within a column of variables using a dummy variable assignment. Or if more than one variable within a column considered significant but all others not significant than all other variables were recoded as Other.

```{r}
library(fastDummies)
library(gridExtra)

#Created new df for train for consolidating columns with dummy variables
train.new <- train

#Creates a column based on Neighborhood that will be used for dummy "other" variables.
train.new$Neighborhood.dummy <- train.new$Neighborhood

#Alters the Neighborhood.dummy column to keep NridgHt, NoRidge, and StoneBr while coverting all other neighborhood variables into "Other"
train.new$Neighborhood.dummy <- as.factor(ifelse(train.new$Neighborhood == "NridgHt", "NridgHt", ifelse(train.new$Neighborhood == "NoRidge", "NoRidge", (ifelse(train.new$Neighborhood == "StoneBr", "StoneBr", "Other")))))

#Checks the class and views the altered Neighborhood.dummy column to be sure changes have properly be made
class(train.new$Neighborhood.dummy)
train.new$Neighborhood.dummy

#Creates a column based on the BldgType feature that contains building types with TwnhsE and Other as the variables. Similar to a yes or no dummy column
train.new$BldgTypeTwnhsE <- as.factor(ifelse(train.new$BldgType == "TwnhsE", "TwnhsE", "Other"))

#Creates a RootMat.dummy column based on the RoofMat feature with only CompShg, Tar&Grv, WdShngl variables and all other variables renamed "Other"
train.new$RoofMat.dummy <- as.factor(ifelse(train.new$RoofMatl == "CompShg", "CompShg", ifelse(train.new$RoofMatl == "Tar&Grv", "Tar&Grv", (ifelse(train.new$RoofMatl == "WdShngl", "WdShngl", "Other")))))

#Creates a Exterior1st.dummy column based on the Exterior1st feature with only BrkFace, Tar&CBlock variables and all other variables renamed "Other"
train.new$Exterior1st.dummy <- as.factor(ifelse(train.new$Exterior1st == "BrkFace", "BrkFace", ifelse(train.new$Exterior1st == "CBlock", "CBlock", "Other")))

#Creates a column based on the BsmtFinType1 feature that contains basement types with Unf and Other as the variables. Similar to a yes or no dummy column
train.new$BsmtFinType1Unf <- as.factor(ifelse(train.new$BsmtFinType1 == "Unf", "Unf", "Other"))

#Creates a column based on the HeatingQC feature that contains heating types with Po and Other as the variables. Similar to a yes or no dummy column
train.new$HeatingQCPo <- as.factor(ifelse(train.new$HeatingQC == "Po", "Po", "Other"))

#Creates a column based on the Functional feature that contains functional types with Typ and Other as the variables. Similar to a yes or no dummy column
train.new$FunctionalTyp <- as.factor(ifelse(train.new$Functional == "Typ", "Typ", "Other"))

#Checks new columns to assure correctness
head(train.new)

#Fit backward inspired model with newly configured and consolidated feature columns
backward.enhanced <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+OverallCond+YearBuilt+RoofMat.dummy+Exterior1st.dummy+BsmtFinType1Unf+BsmtFinSF1+HeatingQCPo+FunctionalTyp+Fireplaces, data = train.new)

#Diagnostics and plotting of enhanced backward model
summary(backward.enhanced)
plot(backward.enhanced)


```

After consolidating the backwards regression features, enhanced backwards model diagnostics showed features that may not be significantly impacting the regressing model. Based on p-value significance the following features were removed from the fit model to simplify it: 
OverallCond
HeatingQCPo
FunctionalTyp

```{r backward enhanced model 2}
#Fit backward enhanded 2 model with above features removed
backward.enhanced2 <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+RoofMat.dummy+Exterior1st.dummy+BsmtFinType1Unf+BsmtFinSF1+Fireplaces, data = train.new)

#Diagnoistics and plot
summary(backward.enhanced2)
plot(backward.enhanced2)
```

Continued elimination of statistically insignificant p-value features from model, removed:
Roofmat.dummy
Exterior1st.dummy
BsmtFinType1Unf

```{r backward enhanced model 3}
#Used code below to experiment with altering the dummy variables for neighborhood to make only have signifigant p-values for neighborhood otherwise changed to other. No successful gains on adjust r squared with changes. Abondaned Neighborhood alterations. 
train.new$Neighborhood.dummy2 <- as.factor(ifelse(train.new$Neighborhood == "NridgHt", "NridgHT_NoRidge_StoneBr", ifelse(train.new$Neighborhood == "NoRidge", "NridgHT_NoRidge_StoneBr", (ifelse(train.new$Neighborhood == "StoneBr", "NridgHT_NoRidge_StoneBr", "Other")))))

#Seven feature linear model nearly as significant as the backward eighteen feature suggested model after dummy variables and consolidation employed. Accuracy gains through including the additional features were negligable compared to the simpler model listed below
backward.enhanced3 <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+BsmtFinSF1+Fireplaces, data = train.new)

#Diagnostics and plotting of final backward inspired model
summary(backward.enhanced3)
plot(backward.enhanced3)
```

The following code adds the matching feature columns present in the new Train data to the new Test data for testing. 
```{r Test Data Feature Matching to Train}
#Create dataframe based on test data
test.new <- test

#Creates new neighborhood feature based on train.new dataframe 
test.new$Neighborhood.dummy <- as.factor(ifelse(test.new$Neighborhood == "NridgHt", "NridgHt", ifelse(test.new$Neighborhood == "NoRidge", "NoRidge", (ifelse(test.new$Neighborhood == "StoneBr", "StoneBr", "Other")))))

#Creates new BldgType feature based on train.new dataframe 
test.new$BldgTypeTwnhsE <- as.factor(ifelse(test.new$BldgType == "TwnhsE", "TwnhsE", "Other"))

#Predict function based on final backward model to test.new data
mod.pred <- predict(backward.enhanced3, test.new)

#Summary and plot
summary(mod.pred)
plot(mod.pred)

#Testing actual plots to predictions
actuals_preds <- data.frame(cbind(actuals=test.new$SalePrice, predicteds=mod.pred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)
head(correlation_accuracy)

backward.enhanced3.fits<-data.frame(fitted.values=backward.enhanced3$fitted.values,residuals=backward.enhanced3$residuals)

#Residual vs Fitted
plot1<-ggplot(backward.enhanced3.fits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(backward.enhanced3.fits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(backward.enhanced3.fits$residuals), slope = sd(backward.enhanced3.fits$residuals))

#Histogram of residuals
plot3<-ggplot(backward.enhanced3.fits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=102,color="blue", fill="red")+
  geom_density(alpha=.99, fill="red")

grid.arrange(plot1, plot2, plot3, ncol=3)
```

Ideal backward inspired model:
LotArea
Neighborhood with options being NridgHt, NoRidge, StoneBr, and all others labeled Others
BldgTypeTwnhsE
OverallQual
YearBuilt
BsmtFinSF1
Fireplaces


```{r test data anaylsis}

#Fit best backward model against test data
test.backward <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+BsmtFinSF1+Fireplaces, data = test.new)

#Summary of results and plot.
summary(test.backward)
plot(test.backward)

#Residuals
test.backward.fits<-data.frame(fitted.values=test.backward$fitted.values,residuals=test.backward$residuals)

#Residual vs Fitted
test.plot1<-ggplot(test.backward.fits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
test.plot2<-ggplot(test.backward.fits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(test.backward.fits$residuals), slope = sd(test.backward.fits$residuals))

#Histogram of residuals
test.plot3<-ggplot(test.backward.fits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=102,color="blue", fill="red")+
  geom_density(alpha=.99, fill="red")

grid.arrange(test.plot1, test.plot2, test.plot3, ncol=3)
```
The following code was to compare Stepwise regression to Forward and Backward results. Stepwise had underperforming Rsquared so it was abandoned for the backward model. 

```{r Stepwise}

#Stepwise performed for testing purposes. Best model option was backward regression. Left here as a history of attempts.

#Stepwise
reg.step<-regsubsets(SalePrice~.,data=train, method = "seqrep", really.big = T, nvmax = 20)

coef(reg.step,15)

summary(reg.step)$adjr2
summary(reg.step)$rss
summary(reg.step)$bic


par(mfrow=c(1,3))
bics<-summary(reg.step)$bic
plot(1:21,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.step)$adjr2
plot(1:21,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.step)$rss
plot(1:21,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

stepwise.model <- train(SalePrice ~., data = train, method = "leapSeq", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

#Graphs to Feature impact on MSE, RMSE, and Rsquared based on Trainining Data
step.results <- stepwise.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

step.results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

step.results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
stepwise.model$bestTune
##summary(forward.model$finalModel)
coef(stepwise.model$finalModel, 16)

```
The following data transforms the SalePrice data with log tranformation to improve residual diagnostics for the model fit. QQ and scatter residual plots show improvement after running backwards model against log transformed data. 

```{r Log Transformed SalePrice Data}
#Log transformation of Sale Price
train$logSalePrice <- log(train$SalePrice)
train.new$logSalePrice <- log(train.new$SalePrice)

#Fit of backward model to log transformated train data
logbackward.model <- train(logSalePrice ~., data = train, method = "leapBackward", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

#Graphs to Feature impact on MSE, RMSE, and Rsquared based on Trainining Data.
#Backward Model MSE
logback.results <- logbackward.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

#Backward Model RMSE
logback.results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

#Backward Model RSquared
logback.results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
logbackward.model$bestTune
##summary(forward.model$finalModel)
coef(logbackward.model$finalModel, 18)

#Seven feature linear model nearly as significant as the backward eighteen feature suggested model after dummy variables and consolidation employed. Accuracy gains through including the additional features were negligable compared to the simpler model listed below
backward.enhanced4 <- lm(logSalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+BsmtFinSF1+Fireplaces, data = train.new)

#Diagnostics and plotting of final backward inspired model
summary(backward.enhanced4)
plot(backward.enhanced4)

#Log Transform test.new
test.new$logSalePrice <- log(test.new$SalePrice)

#Predict function based on final backward model to test.new data
logmod.pred <- predict(backward.enhanced4, test.new)

#Summary and plot
summary(mod.pred)
plot(mod.pred)

#Testing actual plots to predictions
actuals_preds <- data.frame(cbind(actuals=test.new$logSalePrice, predicteds=logmod.pred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)
head(correlation_accuracy)

backward.enhanced4.fits<-data.frame(fitted.values=backward.enhanced4$fitted.values,residuals=backward.enhanced4$residuals)

#Residual vs Fitted
plot7<-ggplot(backward.enhanced4.fits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot8<-ggplot(backward.enhanced3.fits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(backward.enhanced3.fits$residuals), slope = sd(backward.enhanced3.fits$residuals))

#Histogram of residuals
plot9<-ggplot(backward.enhanced3.fits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=102,color="blue", fill="red")+
  geom_density(alpha=.99, fill="red")

grid.arrange(plot7, plot8, plot9, ncol=3)
```
The following code takes the log transformed data, applies the transformation to the SalePrice data and runs the model against the test data after tranformation. Residual plots improve as with the train model run. Histogram still shows some right skewing and the QQ plot still has some abnormal tailing but less so when compared with non-transformation data set. 

```{r logSalePrice Test}

#Model fit to log transformed test data
logtest.backward <- lm(logSalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+BsmtFinSF1+Fireplaces, data = test.new)

#Summary and plot
summary(logtest.backward)
plot(logtest.backward)

#Residuals
logtest.backward.fits<-data.frame(fitted.values=test.backward$fitted.values,residuals=test.backward$residuals)

#Residual vs Fitted
test.plot4<-ggplot(logtest.backward.fits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+ xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
test.plot5<-ggplot(logtest.backward.fits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(test.backward.fits$residuals), slope = sd(test.backward.fits$residuals))

#Histogram of residuals
test.plot6<-ggplot(logtest.backward.fits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=102,color="blue", fill="red")+
  geom_density(alpha=.99, fill="red")

grid.arrange(test.plot4, test.plot5, test.plot6, ncol=3)


```


###Lasso
```{r}
library(glmnet)

# JR - Added line to eliminate NAs.  Not sure if correct solution, but allowed lasso to run.
# https://stackoverflow.com/questions/6447708/model-matrix-generates-fewer-rows-than-original-data-frame
train <- na.omit(train)

test <- na.omit(test)

#Formatting data for GLM net
x=model.matrix(SalePrice~.,train)[,-1]
y=log(train$SalePrice)


xtest<-model.matrix(SalePrice~.,test)[,-1]
ytest<-log(test$SalePrice)

#dim(train)
#str(train)
#str(x)
#str(y)

grid=10^seq(10,-2, length =730)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)


bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)
```


####Update 2 two way anova####
#Q: check independent assumption of predictors?
```{r}
#create a data frame with OUTPUT SalePrice AND two catagorical predictors Neighborhood 25 levels, HouseStyle 8 levels for two way anova

#dim(Housing)
#str(Housing)
#which( colnames(HousingData)=="Neighborhood" )

#which( colnames(HousingData)=="HouseStyle" )
#which( colnames(HousingData)=="SalePrice" )

HousingANOVA <- Housing[, c(10,13,71)]

str(HousingANOVA )
head(HousingANOVA )

```

#Provide a means plot of the data.
```{r}
#Provide a means plot of the data.
attach(HousingANOVA)
mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/sqrt(length(x)),min(x), max(x), IQR(x))
  names(result)<-c("N","Mean","SD","SE","MIN","MAX", "IQR")
  return(result)
}
sumstats<-aggregate(SalePrice~Neighborhood*HouseStyle,data=HousingANOVA,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1)
  
#SD as error bar
  ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1)
  
  

```
#### Fit a nonadditive 2 way anova model to the data set and provide the residual diagnostics. 
```{r}

  #Fit a nonadditive 2 way anova model to the data set and provide the residual diagnostics.
  model.fit<-aov(SalePrice~Neighborhood+HouseStyle+Neighborhood*HouseStyle,data=HousingANOVA)
par(mfrow=c(1,2))
plot(model.fit$fitted.values,model.fit$residuals,ylab="Resdiduals",xlab="Fitted")
qqnorm(model.fit$residuals)

#residuals look fan out--non-constant variance, QQplot does not look normal, so decide to log transform SalePrice
```

####log transform SalePrice
```{r}
HousingANOVA$LogSalePrice=log(HousingANOVA$SalePrice)
str(HousingANOVA )
head(HousingANOVA )

```

###mean plot of transformed data
```{r}
#Provide a means plot of the data.
attach(HousingANOVA)
mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/sqrt(length(x)),min(x), max(x), IQR(x))
  names(result)<-c("N","Mean","SD","SE","MIN","MAX", "IQR")
  return(result)
}
sumstats<-aggregate(LogSalePrice~Neighborhood*HouseStyle,data=HousingANOVA,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1)
  
#SD as error bar
  ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1)
```
#fit
#Q: how to check for outlier?
```{r}
 #Fit a nonadditive 2 way anova model to transformed data and provide the residual diagnostics.
  model.fit<-aov(LogSalePrice~Neighborhood+HouseStyle+Neighborhood*HouseStyle,data=HousingANOVA)
par(mfrow=c(1,2))
plot(model.fit$fitted.values,model.fit$residuals,ylab="Resdiduals",xlab="Fitted")
qqnorm(model.fit$residuals)

# the normality assumption met after log transformation of y

```

####First layer of test. High level test. Provide the type 3 ANOVA F-tests. Type III SS tested on interactions (the presence of a main effect after the other main effect and interaction)
```{r}
#provide the type 3 ANOVA F-tests
library(car)
Anova(model.fit,type=3)
```
####Mutiple testing corrections

```{r}
TukeyHSD(model.fit,"Background",conf.level=.95)
```
####Second layer of test. write contrast. 
```{r}

```
