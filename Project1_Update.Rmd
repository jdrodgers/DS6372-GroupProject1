---
title: "Project_1"
author: "Charles, Lei, John"
date: "2/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(leaps)
library(caret)
library(car)
library(MASS)
library(ggvis)


#Set path to housing data.
# JR - Changed path to be relative path assuming rproj file in same directory as rmd files. 
#       Should help with running it on separate workstations
Project <- "/Users/Chase/Desktop/MSDS/MSDS6306/Homework/SMU_MSDS_Homework/Case_Study_1/DS6372-GroupProject1/ImportedFiles/train_clean_final.csv"

HousingData <- read.csv(Project)
dim(HousingData)

```
```{r}
####UPDATE 1 EDA ####
#Too many missing values, remove: in LotFrontage 259, FirePlaceQu 690, PoolQC 1453,MiscFeature 1406
# Fence 1179, actually not missing value but one level of factor, need transform. or how to deal with it??suggestion? remove for now for further analysis.
#These have less than  100 missing values, keep in dataset: Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, MasVnrArea 8


#Report NA count, put in data frame for easier display options using kable
naCountDF <-as.data.frame(sapply(HousingData, function(x) sum(length(which(is.na(x))))))
names(naCountDF) <- ("NA Count") # rename columns

# Print the NA Counts by State

library(kableExtra)
kable(naCountDF,format="html", caption="Table -NA Count (NA=blanks)",align='c') %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

```

####EDA, find out the column index for "FireplaceQu"
```{r}
#str(HousingData)

which( colnames(HousingData)=="FireplaceQu" )

```
####Removes unnessary predictor columns from dataset
```{r}

Housing <- HousingData[, -c(1,4,7,15,58,73:76,79)] #Removes unnessary predictor columns from dataset
#Features removed: Id, Alley, Condition2, PoolQC, Fense, MiscFeature, MiscVal, SaleType

#4 LotFrontage too mnay missing data  259
#FireplaceQu has need to be removed

#dim(Housing)
#(1460   71)

head(Housing)
tail(Housing)
```
####EDA
```{r}
#str(Housing)
```

```{r Train and Test Samples}
#Following code creates training and test samples. 
set.seed(1234)
index<-sample(1:dim(Housing)[1],nrow(Housing)/2,replace=F)
train<-Housing[index,]
test<-Housing[-index,]


#Forward Model
reg.fwd<-regsubsets(SalePrice~.,data=train, method = "forward", really.big = T, nvmax = 20)
coef(reg.fwd, 19)


```

```{r}
#Alternate Forward Model with better results accessibility
train.control <- trainControl(method = "cv", number = 5)

forward.model <- train(SalePrice ~., data = train, method = "leapForward", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

names(results)

#Graphs to Feature impact on MSE, RMSE, and Rsquared based on Trainining Data
results <- forward.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
forward.model$bestTune
##summary(forward.model$finalModel)
coef(forward.model$finalModel, 16)

##Uses other reg.fwd model to sanity check the models, same features recommended in either case. 
coef(reg.fwd, 16)

summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic

par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:21,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:21,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:21,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

```



```{r Backward}

#Backward
reg.back<-regsubsets(SalePrice~.,data=train, method = "backward", really.big = T, nvmax = 20)

coef(reg.back,3)

summary(reg.back)$adjr2
summary(reg.back)$rss
summary(reg.back)$bic


par(mfrow=c(1,3))
bics<-summary(reg.back)$bic
plot(1:21,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.back)$adjr2
plot(1:21,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.back)$rss
plot(1:21,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)


backward.model <- train(SalePrice ~., data = train, method = "leapBackward", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

#Graphs to Feature impact on MSE, RMSE, and Rsquared based on Trainining Data
back.results <- backward.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

back.results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

back.results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
backward.model$bestTune
##summary(forward.model$finalModel)
coef(backward.model$finalModel, 18)

##Uses other reg.fwd model to sanity check the models, same features recommended in either case. 
coef(reg.back, 18)



```


Backward regression model with 18 predictors had greatest Rsquared return in comparison with stepwise and forward. The following features were output. 

LotArea 
NeighborhoodNoRidge 
NeighborhoodNridgHt 
NeighborhoodStoneBr 
BldgTypeTwnhsE 
OverallQual 
OverallCond 
YearBuilt 
RoofMatlCompShg 
RoofMatlTar&Grv 
RoofMatlWdShngl 
Exterior1stBrkFace 
Exterior1stCBlock 
BsmtFinType1Unf 
BsmtFinSF1 
HeatingQCPo 
FunctionalTyp 
Fireplaces

```{r}
library(fastDummies)
library(gridExtra)

#Created new df for train for consolidating columns with dummy variables
train.new <- train

train.new$Neighborhood.dummy <- train.new$Neighborhood


train.new$Neighborhood.dummy <- as.factor(ifelse(train.new$Neighborhood == "NridgHt", "NridgHt", ifelse(train.new$Neighborhood == "NoRidge", "NoRidge", (ifelse(train.new$Neighborhood == "StoneBr", "StoneBr", "Other")))))

class(train.new$Neighborhood.dummy)
train.new$Neighborhood.dummy

train.new$BldgTypeTwnhsE <- as.factor(ifelse(train.new$BldgType == "TwnhsE", "TwnhsE", "Other"))

train.new$RoofMat.dummy <- as.factor(ifelse(train.new$RoofMatl == "CompShg", "CompShg", ifelse(train.new$RoofMatl == "Tar&Grv", "Tar&Grv", (ifelse(train.new$RoofMatl == "WdShngl", "WdShngl", "Other")))))

train.new$Exterior1st.dummy <- as.factor(ifelse(train.new$Exterior1st == "BrkFace", "BrkFace", ifelse(train.new$Exterior1st == "CBlock", "CBlock", "Other")))

train.new$BsmtFinType1Unf <- as.factor(ifelse(train.new$BsmtFinType1 == "Unf", "Unf", "Other"))

train.new$HeatingQCPo <- as.factor(ifelse(train.new$HeatingQC == "Po", "Po", "Other"))

train.new$FunctionalTyp <- as.factor(ifelse(train.new$Functional == "Typ", "Typ", "Other"))

head(train.new)

backward.enhanced <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+OverallCond+YearBuilt+RoofMat.dummy+Exterior1st.dummy+BsmtFinType1Unf+BsmtFinSF1+HeatingQCPo+FunctionalTyp+Fireplaces, data = train.new)

summary(backward.enhanced)
plot(backward.enhanced)


```

After consolidating the backwards regression suggested features went through removing features with a statistically insignificant p-value, removed:
OverallCond
HeatingQCPo
FunctionalTyp

```{r backward enhanced model 2}
backward.enhanced2 <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+RoofMat.dummy+Exterior1st.dummy+BsmtFinType1Unf+BsmtFinSF1+Fireplaces, data = train.new)

summary(backward.enhanced2)
plot(backward.enhanced2)
```

Continued removed statistically insignificant p-value features from model, removed:
Roofmat.dummy
Exterior1st.dummy
BsmtFinType1Unf

```{r backward enhanced model 3}
#Used code below to experiment with altering the dummy variables for neighborhood to make only have signifigant p-values for neighborhood otherwise changed to other. No successful gains on adjust r squared with changes. 
train.new$Neighborhood.dummy2 <- as.factor(ifelse(train.new$Neighborhood == "NridgHt", "NridgHT_NoRidge_StoneBr", ifelse(train.new$Neighborhood == "NoRidge", "NridgHT_NoRidge_StoneBr", (ifelse(train.new$Neighborhood == "StoneBr", "NridgHT_NoRidge_StoneBr", "Other")))))

#Seven feature linear model nearly as significant as the backward eighteen feature suggested model after dummy variables and consolidation employed.
backward.enhanced3 <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+BsmtFinSF1+Fireplaces, data = train.new)

summary(backward.enhanced3)
plot(backward.enhanced3)

test.new <- test
test.new$Neighborhood.dummy <- as.factor(ifelse(test.new$Neighborhood == "NridgHt", "NridgHt", ifelse(test.new$Neighborhood == "NoRidge", "NoRidge", (ifelse(test.new$Neighborhood == "StoneBr", "StoneBr", "Other")))))

test.new$BldgTypeTwnhsE <- as.factor(ifelse(test.new$BldgType == "TwnhsE", "TwnhsE", "Other"))

mod.pred <- predict(backward.enhanced3, test.new)

summary(mod.pred)
plot(mod.pred)

actuals_preds <- data.frame(cbind(actuals=test.new$SalePrice, predicteds=mod.pred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)
head(correlation_accuracy)

head(backward.enhanced3)

backward.enhanced3.fits<-data.frame(fitted.values=backward.enhanced3$fitted.values,residuals=backward.enhanced3$residuals)

#Residual vs Fitted
plot1<-ggplot(backward.enhanced3.fits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(backward.enhanced3.fits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(backward.enhanced3.fits$residuals), slope = sd(backward.enhanced3.fits$residuals))

#Histogram of residuals
plot3<-ggplot(backward.enhanced3.fits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=102,color="blue", fill="red")+
  geom_density(alpha=.99, fill="red")

grid.arrange(plot1, plot2, plot3, ncol=3)
```

Ideal backward inspired model:
LotArea
Neighborhood with options being NridgHt, NoRidge, StoneBr, and all others labeled Others
BldgTypeTwnhsE
OverallQual
YearBuilt
BsmtFinSF1
Fireplaces


```{r test data anaylsis}

test.backward <- lm(SalePrice~LotArea+Neighborhood.dummy+BldgTypeTwnhsE+OverallQual+YearBuilt+BsmtFinSF1+Fireplaces, data = test.new)

summary(test.backward)
plot(test.backward)

test.backward.fits<-data.frame(fitted.values=test.backward$fitted.values,residuals=test.backward$residuals)

#Residual vs Fitted
test.plot1<-ggplot(test.backward.fits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
test.plot2<-ggplot(test.backward.fits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(test.backward.fits$residuals), slope = sd(test.backward.fits$residuals))

#Histogram of residuals
test.plot3<-ggplot(test.backward.fits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=102,color="blue", fill="red")+
  geom_density(alpha=.99, fill="red")

grid.arrange(test.plot1, test.plot2, test.plot3, ncol=3)
```


```{r Stepwise}

#Stepwise performed for testing purposes. Best model option was backward regression. Left here as a history of attempts.

#Stepwise
reg.step<-regsubsets(SalePrice~.,data=train, method = "seqrep", really.big = T, nvmax = 20)

coef(reg.step,15)

summary(reg.step)$adjr2
summary(reg.step)$rss
summary(reg.step)$bic


par(mfrow=c(1,3))
bics<-summary(reg.step)$bic
plot(1:21,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.step)$adjr2
plot(1:21,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.step)$rss
plot(1:21,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

stepwise.model <- train(SalePrice ~., data = train, method = "leapSeq", tuneGrid = data.frame(nvmax = 1:20),trControl = train.control, na.action = na.exclude)

#Graphs to Feature impact on MSE, RMSE, and Rsquared based on Trainining Data
step.results <- stepwise.model$results
results %>%
    ggvis(x=~ nvmax, y=~RMSE^2) %>%
  layer_points(fill= ~ RMSE^2, size = ~ RMSE^2) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "MSE") %>%
  add_axis("x", title = "Features")

step.results %>%
    ggvis(x=~ nvmax, y=~RMSE) %>%
  layer_points(fill= ~ RMSE, size = ~ RMSE) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "RMSE") %>%
  add_axis("x", title = "Features")

step.results %>%
    ggvis(x=~ nvmax, y=~Rsquared) %>%
  layer_points(fill= ~ Rsquared, size = ~ Rsquared) %>%
  layer_lines(stroke := "red") %>%
  add_axis("y", title = "Rsquared") %>%
  add_axis("x", title = "Features")

#Suggests ideal feature number
stepwise.model$bestTune
##summary(forward.model$finalModel)
coef(stepwise.model$finalModel, 16)

```


###Lasso
```{r}
library(glmnet)

# JR - Added line to eliminate NAs.  Not sure if correct solution, but allowed lasso to run.
# https://stackoverflow.com/questions/6447708/model-matrix-generates-fewer-rows-than-original-data-frame
train <- na.omit(train)

test <- na.omit(test)

#Formatting data for GLM net
x=model.matrix(SalePrice~.,train)[,-1]
y=log(train$SalePrice)


xtest<-model.matrix(SalePrice~.,test)[,-1]
ytest<-log(test$SalePrice)

#dim(train)
#str(train)
#str(x)
#str(y)

grid=10^seq(10,-2, length =730)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)


bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)
```


####Update 2 two way anova####
#Q: check independent assumption of predictors?
```{r}
#create a data frame with OUTPUT SalePrice AND two catagorical predictors Neighborhood 25 levels, HouseStyle 8 levels for two way anova

#dim(Housing)
#str(Housing)
#which( colnames(HousingData)=="Neighborhood" )

#which( colnames(HousingData)=="HouseStyle" )
#which( colnames(HousingData)=="SalePrice" )

HousingANOVA <- Housing[, c(10,13,71)]

str(HousingANOVA )
head(HousingANOVA )

```

#Provide a means plot of the data.
```{r}
#Provide a means plot of the data.
attach(HousingANOVA)
mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/sqrt(length(x)),min(x), max(x), IQR(x))
  names(result)<-c("N","Mean","SD","SE","MIN","MAX", "IQR")
  return(result)
}
sumstats<-aggregate(SalePrice~Neighborhood*HouseStyle,data=HousingANOVA,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1)
  
#SD as error bar
  ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1)
  
  

```
#### Fit a nonadditive 2 way anova model to the data set and provide the residual diagnostics. 
```{r}

  #Fit a nonadditive 2 way anova model to the data set and provide the residual diagnostics.
  model.fit<-aov(SalePrice~Neighborhood+HouseStyle+Neighborhood*HouseStyle,data=HousingANOVA)
par(mfrow=c(1,2))
plot(model.fit$fitted.values,model.fit$residuals,ylab="Resdiduals",xlab="Fitted")
qqnorm(model.fit$residuals)

#residuals look fan out--non-constant variance, QQplot does not look normal, so decide to log transform SalePrice
```

####log transform SalePrice
```{r}
HousingANOVA$LogSalePrice=log(HousingANOVA$SalePrice)
str(HousingANOVA )
head(HousingANOVA )

```

###mean plot of transformed data
```{r}
#Provide a means plot of the data.
attach(HousingANOVA)
mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/sqrt(length(x)),min(x), max(x), IQR(x))
  names(result)<-c("N","Mean","SD","SE","MIN","MAX", "IQR")
  return(result)
}
sumstats<-aggregate(LogSalePrice~Neighborhood*HouseStyle,data=HousingANOVA,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1)
  
#SD as error bar
  ggplot(sumstats,aes(x=Neighborhood,y=Mean,group=HouseStyle,colour=HouseStyle))+
  ylab("SalePrice")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1)
```
#fit
#Q: how to check for outlier?
```{r}
 #Fit a nonadditive 2 way anova model to transformed data and provide the residual diagnostics.
  model.fit<-aov(LogSalePrice~Neighborhood+HouseStyle+Neighborhood*HouseStyle,data=HousingANOVA)
par(mfrow=c(1,2))
plot(model.fit$fitted.values,model.fit$residuals,ylab="Resdiduals",xlab="Fitted")
qqnorm(model.fit$residuals)

# the normality assumption met after log transformation of y

```

####First layer of test. High level test. Provide the type 3 ANOVA F-tests. Type III SS tested on interactions (the presence of a main effect after the other main effect and interaction)
```{r}
#provide the type 3 ANOVA F-tests
library(car)
Anova(model.fit,type=3)
```
####Mutiple testing corrections

```{r}
TukeyHSD(model.fit,"Background",conf.level=.95)
```
####Second layer of test. write contrast. 
```{r}

```
